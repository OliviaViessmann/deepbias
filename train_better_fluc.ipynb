{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for improved fMRI-bias learning\n",
    "#OV Jan 2021\n",
    "####IMPORTS\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import numpy as np\n",
    "import keras\n",
    "import freesurfer as fs\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import layers as KL\n",
    "#Load my own functions\n",
    "import my_utils as utils\n",
    "#from freesurfer import deeplearn as fsd\n",
    "#OV: This imports all the local settings, e.g. subjects etc. from the header.py and netparms.py file, make your changes in there as needed\n",
    "from netparms import *\n",
    "from header_better import *\n",
    "import losspad\n",
    "#OV add matplotlib for plotting and cross checks\n",
    "#import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import string as string\n",
    "####END IMPORTS\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IN DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfs_sphere   = np.load('data_train/surfs_sphere.npy',allow_pickle = \"true\")\n",
    "surfs_pials    = np.load('data_train/surfs_pials.npy',allow_pickle = \"true\")\n",
    "surfs_inflated = np.load('data_train/surfs_inflated.npy',allow_pickle = \"true\")\n",
    "labels_cortex  = np.load('data_train/labels_cortex.npy',allow_pickle = \"true\")\n",
    "maps_func      = np.load('data_train/maps_func.npy',allow_pickle = \"true\")\n",
    "maps_angles    = np.load('data_train/maps_angles.npy',allow_pickle = \"true\")\n",
    "maps_thickness = np.load('data_train/maps_thickness.npy',allow_pickle = \"true\")\n",
    "maps_angio     = np.load('data_train/maps_angio.npy',allow_pickle = \"true\")\n",
    "NumFuncs       = np.load('data_train/NumFuncs.npy',allow_pickle = \"true\")\n",
    "#%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define in and ouput data and assign a cortex label if wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numInputs = 3\n",
    "#Percentile for removal of outliers\n",
    "percentiles = [3,97]\n",
    "#Assign in- and output data\n",
    "inputs  = []\n",
    "outputs = [] \n",
    "input_shape  = (maps_thickness[0][0].shape+(numInputs,))\n",
    "\n",
    "for sno, subject in enumerate(subjects): \n",
    "   for hno, hemi in enumerate(hemis):\n",
    "      for rn, func_runs in enumerate(Func_runs):\n",
    "         if numInputs > 1:\n",
    "            #We don't have a full set of runs for all subjects, some didn't register well\n",
    "            #also multiply by cortex label \n",
    "            tmp_in = np.zeros(input_shape)\n",
    "            try:\n",
    "               #remove outliers and then normalise fluctuations and multiply by 100 \n",
    "               tmp = maps_func[sno][hno][rn].data\n",
    "               tmp = utils.zero_outliers_image(tmp,percentiles)\n",
    "               tmp = utils.normalize_image(tmp)*100\n",
    "               tmp = np.multiply(tmp,labels_cortex[sno][hno])\n",
    "               outputs.append(tmp)\n",
    "               tmp_in[...,0] = np.multiply(maps_thickness[sno][hno].data,labels_cortex[sno][hno])\n",
    "               tmp_in[...,1] = np.multiply(maps_angio[sno][hno].data,labels_cortex[sno][hno])\n",
    "               tmp_in[...,2] = np.multiply(maps_angles[sno][hno][rn].data,labels_cortex[sno][hno])\n",
    "               inputs.append(tmp_in)\n",
    "            except IndexError:\n",
    "               continue\n",
    "         elif numInputs == 1:\n",
    "            try: \n",
    "               #remove outliers and then normalise fluctuations and multiply by 100 \n",
    "               tmp = maps_func[sno][hno][rn].data\n",
    "               tmp = utils.zero_outliers_image(tmp,percentiles)\n",
    "               tmp = utils.normalize_image(tmp)*100\n",
    "               tmp = np.multiply(tmp,labels_cortex[sno][hno])\n",
    "               outputs.append(tmp)\n",
    "               #inputs.append(np.multiply(maps_angio[sno][hno].data,labels_cortex[sno][hno]))\n",
    "               inputs.append(np.multiply(maps_angles[sno][hno][rn].data,labels_cortex[sno][hno]))\n",
    "               #inputs.append(np.multiply(maps_thickness[sno][hno].data,labels_cortex[sno][hno]))\n",
    "            except IndexError:\n",
    "               continue  \n",
    "print(NumFuncs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = utils.zero_outliers_image(maps_angles[0][0][0].data,percentiles)\n",
    "plt.figure()\n",
    "plt.imshow(tmp, cmap = 'jet')\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.imshow(maps_func[0][0][0], cmap = 'jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circuarly leave-on-out strategy for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define training settings\n",
    "epochs        = 200\n",
    "learning_rate = 1e-3\n",
    "use_mask      = True\n",
    "\n",
    "#Initialize\n",
    "loss           = []\n",
    "val_loss       = []\n",
    "Prediction_err = []\n",
    "NullOutput_err = []\n",
    "NullInput_err  = []\n",
    "plt.figure()\n",
    "\n",
    "NumFuncs    = NumFuncs.astype(int)\n",
    "CumSumFuncs = np.cumsum(NumFuncs)\n",
    "for subj, subject in enumerate(subjects):\n",
    "   target_shape = (maps_func[0][0][0].shape+(1,)) \n",
    "   input_shape  = (maps_thickness[0][0].shape+(numInputs,))\n",
    "   print('subject: %s' %(subject))\n",
    "  \n",
    "   #OV: neuron (ne) expects another channel, but here we just have one number (the thickness or fmri derived parameter), \n",
    "   #maybe replace that with the time series itself? for now we just add np.newaxis as an empty channel\n",
    "   #Assign training and testing data depending on subject\n",
    "   if numInputs > 1: \n",
    "      #print(CumSumFuncs[subj-1],CumSumFuncs[subj])\n",
    "      if (subj > 0 and subj<(len(subjects)-1)):\n",
    "        xtrain    = np.array(np.concatenate((inputs[0 : CumSumFuncs[subj-1]],\\\n",
    "                                        inputs[CumSumFuncs[subj]:-1])))\n",
    "        ytrain    = np.array(np.concatenate((outputs[0 : CumSumFuncs[subj-1]],\\\n",
    "                                        outputs[CumSumFuncs[subj]:-1])))[...,np.newaxis]\n",
    "        xtest     = np.array(inputs[CumSumFuncs[subj-1] : CumSumFuncs[subj]])\n",
    "        ytest     = np.array(outputs[CumSumFuncs[subj-1] : CumSumFuncs[subj]])[...,np.newaxis]\n",
    "      elif subj == 0:\n",
    "        xtrain    = np.array(inputs[NumFuncs[1]:-1])\n",
    "        ytrain    = np.array(outputs[NumFuncs[1]:-1])[...,np.newaxis]\n",
    "        xtest     = np.array(inputs[0:NumFuncs[0]])\n",
    "        ytest     = np.array(outputs[0:NumFuncs[0]])[...,np.newaxis]\n",
    "      elif subj == (len(subjects)-1):\n",
    "        xtrain    = np.array(inputs[0:CumSumFuncs[subj-2]])\n",
    "        ytrain    = np.array(outputs[0:CumSumFuncs[subj-2]])[...,np.newaxis]\n",
    "        xtest     = np.array(inputs[CumSumFuncs[subj-1]:CumSumFuncs[subj]])\n",
    "        ytest     = np.array(outputs[CumSumFuncs[subj-1]:CumSumFuncs[subj]])[...,np.newaxis]\n",
    "                                     \n",
    "   elif numInputs == 1:\n",
    "      #print(CumSumFuncs[subj-1],CumSumFuncs[subj])\n",
    "      if (subj > 0 and subj<(len(subjects)-1)):\n",
    "        xtrain    = np.array(np.concatenate((inputs[0 : CumSumFuncs[subj-1]],\\\n",
    "                                        inputs[CumSumFuncs[subj]:-1])))[...,np.newaxis]\n",
    "        ytrain    = np.array(np.concatenate((outputs[0 : CumSumFuncs[subj-1]],\\\n",
    "                                        outputs[CumSumFuncs[subj]:-1])))[...,np.newaxis]\n",
    "        xtest     = np.array(inputs[CumSumFuncs[subj-1] : CumSumFuncs[subj]])[...,np.newaxis]\n",
    "        ytest     = np.array(outputs[CumSumFuncs[subj-1] : CumSumFuncs[subj]])[...,np.newaxis]\n",
    "      elif subj == 0:\n",
    "        xtrain    = np.array(inputs[NumFuncs[1]:-1])[...,np.newaxis]\n",
    "        ytrain    = np.array(outputs[NumFuncs[1]:-1])[...,np.newaxis]\n",
    "        xtest     = np.array(inputs[0:NumFuncs[0]])[...,np.newaxis]\n",
    "        ytest     = np.array(outputs[0:NumFuncs[0]])[...,np.newaxis]\n",
    "      elif subj == (len(subjects)-1):\n",
    "        xtrain    = np.array(inputs[0:CumSumFuncs[subj-2]])[...,np.newaxis]\n",
    "        ytrain    = np.array(outputs[0:CumSumFuncs[subj-2]])[...,np.newaxis]\n",
    "        xtest     = np.array(inputs[CumSumFuncs[subj-1]:CumSumFuncs[subj]])[...,np.newaxis] \n",
    "        ytest     = np.array(outputs[CumSumFuncs[subj-1]:CumSumFuncs[subj]])[...,np.newaxis]     \n",
    "\n",
    "   print('input dimensions training: %s and testing %s ' %(xtrain.shape, xtest.shape))\n",
    "   print('output dimensions: training %s and testing: %s ' %(ytrain.shape, ytest.shape))\n",
    "    #sanity check\n",
    "   #xtrain[xtrain>0] = 1\n",
    "   #xtest[xtest>0]=1\n",
    "   #Set up model\n",
    "   K.clear_session()\n",
    "   model = ne.models.unet(nb_features, input_shape, nb_depth, (3,3), 1, nb_conv_per_level = nb_conv_per_level, \\\n",
    "                          batch_norm = -1, final_pred_activation = 'linear')\n",
    "   #make directory for callbacks \n",
    "   model_dir = 'model_dir'\n",
    "   RandomName='fluc_from_triple'.join(rd.sample(string.ascii_lowercase,5))\n",
    "   os.mkdir(os.path.join('/cluster/visuo/users/olivia/Projects/DeepBias/Better_code',model_dir,RandomName))\n",
    "   save_file_name = os.path.join(model_dir, RandomName, '{epoch:02d}.h5') \n",
    "   save_callbacks = tf.keras.callbacks.ModelCheckpoint(save_file_name)\n",
    "   #subclass of lr_callback written by Bruce. If loss explodes it detects that and falls back to a checkpoint\n",
    "   lr_callbacks   = nes.tf.callbacks.ReduceLRWithModelCheckpointAndRecovery(save_file_name, \\\n",
    "                                                                            monitor='loss', \\\n",
    "                                                                            verbose=1, \\\n",
    "                                                                            cooldown=10, \\\n",
    "                                                                            factor=.5, \\\n",
    "                                                                            patience=100, \\\n",
    "                                                                            thres=.2, \\\n",
    "                                                                            save_weights_only=True, \\\n",
    "                                                                            min_lr=1e-7)\n",
    "\n",
    "   sphereloss = losspad.spherical_loss(target_shape[0:2],pad=pad) #_no_pad[0:2],pad=pad)\n",
    "   if use_mask == False:\n",
    "       print('NOT using masked loss')\n",
    "       #OV changed to tensorflow.keras bc it gave attribute error otherwise\n",
    "       model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='MSE', metrics = ['accuracy'])\n",
    "   else:\n",
    "       print('using masked loss')\n",
    "       model.compile(optimizer=Adam(lr=learning_rate), loss=utils.masked_MSE_loss , metrics = ['accuracy'])\n",
    "\n",
    "   #Fit\n",
    "   #dg = generator(xtrain[...,np.newaxis], ytrain, 8,augment_noise=.3)\n",
    "   #Threshold thickness for training, but not for testing data\n",
    "   batch_size = 8\n",
    "   steps_per_epoch = xtrain.shape[0]/batch_size \n",
    "   dg    = utils.generator(xtrain, ytrain, batch_size = batch_size, augment_noise=.3)\n",
    "   fhist = model.fit(dg, \n",
    "                     steps_per_epoch=steps_per_epoch, \\\n",
    "                     epochs=epochs, \\\n",
    "                     validation_data= (xtest, ytest), \\\n",
    "                     verbose=1, \\\n",
    "                     callbacks=[lr_callbacks])\n",
    "\n",
    "   #Saving information for each CNN (i.e. each subject/cross valdiation cycle)\n",
    "   loss.append(fhist.history['loss'])\n",
    "   val_loss.append(fhist.history['val_loss'])\n",
    "   ep = np.arange(epochs)\n",
    "   plt.plot(ep,fhist.history['loss'], label=('train loss ' + subject))\n",
    "   plt.plot(ep,fhist.history['val_loss'],label = ('val loss ' + subject))\n",
    "\n",
    "\n",
    "   #PREDICT\n",
    "   #Test on unseen data from unseen subject\n",
    "   yp                 = model.predict(xtest)\n",
    "   #Take out non cortex labeled areas\n",
    "   if numInputs > 1:\n",
    "      yp[xtest[:,:,:,0]==0]       = 0\n",
    "      ytest[xtest[:,:,:,0]==0]    = 0\n",
    "   else :\n",
    "      yp[xtest==0]       = 0\n",
    "      ytest[xtest==0]    = 0\n",
    "\n",
    "   #Mean differene between predicition and truth\n",
    "   print('Prediciting error is ' + str(np.absolute(ytest-yp).mean()))\n",
    "   Prediction_err.append(np.absolute(ytest-yp).mean())\n",
    "\n",
    "   #Error for scrambled prediction, \"null output error\"\n",
    "   NullOutput   = ytest.copy()\n",
    "   np.random.shuffle(NullOutput)\n",
    "   #np.take(NullOutput,np.random.permutation(NullOutput.shape[2]),axis=2,out=NullOutput)\n",
    "   NullOutput_err.append(np.absolute(ytest - NullOutput).mean())\n",
    "   print('NullOutput error is ' + str(np.absolute(ytest - NullOutput).mean()))\n",
    "\n",
    "   #Error for scrambled predictor (input), \"null input error\"\n",
    "   NullInput    = xtest.copy().squeeze()\n",
    "   np.random.shuffle(NullInput)\n",
    "   #np.take(NullInput,np.random.permutation(NullInput.shape[2]),axis=2,out=NullInput)\n",
    "   yp_NullInput = model.predict(NullInput)\n",
    "   NullInput_err.append(np.absolute(ytest - yp_NullInput).mean())\n",
    "   print('NullInput error is ' + str(np.absolute(ytest -yp_NullInput).mean()))\n",
    "\n",
    "   #Visualize activations\n",
    "   #extracts output of the top 8 layers\n",
    "   #layer_outputs = [layer.output for layer in model.layers]\n",
    "   #Creates a model that returns these outputs, given the model input:\n",
    "   #activation_model = tf.keras.models.Model(inputs=model.input, outputs = layer_outputs)\n",
    "   #this will return  list of 5 numpy arrays, one array per lyer activation\n",
    "   #print(xtest.shape)\n",
    "   #activations = activation_model.predict(xtest[0])\n",
    "   #first_layer_activation = activations[0]\n",
    "   #plt.matshow(first_layer_activation[0,:,:,11], cmap = 'viridis')\n",
    "   #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(xtest[:,:,:,0].shape)\n",
    "test = ytest.copy().squeeze()\n",
    "print(test[1,:10,1])\n",
    "#np.random.shuffle(test)\n",
    "#np.take(test,np.random.shuffle,axis=2,out=test)\n",
    "print(test[1,:10,1])\n",
    "print(ytest[1,:10,1])\n",
    "print(yp[1,:10,1])\n",
    "print(yp_NullInput[1,:10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Histogram of predicition error for orientation (not in percent)\n",
    "fig = plt.figure()\n",
    "tmp = ytest-yp\n",
    "tmp = tmp[~np.isnan(tmp)]\n",
    "tmp = tmp[~np.isinf(tmp)]\n",
    "tmp = tmp[tmp!=0]\n",
    "tmp2 = ytest - NullOutput\n",
    "tmp2 = tmp2[~np.isnan(tmp2)]\n",
    "tmp2 = tmp2[~np.isinf(tmp2)] \n",
    "n,bins,patches = plt.hist(tmp, bins = 'auto', alpha = 0.4, label = 'Prediction')\n",
    "plt.hist(tmp2, bins = bins, alpha = 0.4, label = 'Randomised input')\n",
    "plt.ylim(0,40000)\n",
    "plt.xlim(-30,30)\n",
    "plt.xlabel('prediction error ')\n",
    "plt.ylabel('counts')\n",
    "#plt.xlim(-90,90)\n",
    "plt.legend()\n",
    "plt.title('Prediction error of orientation from fMRI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give out CNN performance results from cross-validation\n",
    "print('CNN average prediction error: '+  str(np.array(Prediction_err).mean()) \\\n",
    "                                      + ' and std dev: ' + str(np.array(Prediction_err).std()))\n",
    "print('CNN average NullInput error: ' +  str(np.array(NullInput_err).mean()) \\\n",
    "                                      + ' and std dev: ' + str(np.array(NullInput_err).std()))\n",
    "print('CNN average NullOutput error: '+  str(np.array(NullOutput_err).mean()) \\\n",
    "                                      + ' and std dev: ' + str(np.array(NullOutput_err).std()))\n",
    "#Show exemplar results\n",
    "sphere      = surfs_sphere[-1][-1]\n",
    "pial_disp   = surfs_inflated[-1][-1]\n",
    "fv          = fs.Freeview()\n",
    "truth       = sphere.sample_parameterization(ytest[-1,::].squeeze())\n",
    "overlay     = fv.OverlayTag(truth, name='truth')#, threshold=(.0, 90)) \n",
    "overlays    = [overlay]\n",
    "prediction  = sphere.sample_parameterization(yp[-1,::].squeeze())\n",
    "overlay     = fv.OverlayTag(prediction, name='prediction')#, threshold=(0, 90)) \n",
    "overlays.append(overlay)\n",
    "difference  = sphere.sample_parameterization(ytest[-1,::].squeeze()-yp[-1,::].squeeze())\n",
    "overlay     = fv.OverlayTag(difference, name='difference')#, threshold=(-50,50)) \n",
    "overlays.append(overlay)\n",
    "\n",
    "\n",
    "fv.surf(pial_disp, overlay=overlays)\n",
    "fv.show(threads=20)\n",
    "#fv.show.__code__.co_varnames\n",
    "\n",
    "#Plot model overview\n",
    "#keras.utils.plot_model(model,to_file='model.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot binned dependencies on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "tmpfunc      = list(itertools.chain(*maps_func[:][:][0][:]))\n",
    "tmpfunc      = list(itertools.chain(*tmpfunc))\n",
    "tmpangles    = list(itertools.chain(*maps_angles[:][:][0][:]))\n",
    "tmpangles    = list(itertools.chain(*tmpangles))\n",
    "tmpfunc      = np.array(tmpfunc)\n",
    "tmpangles    = np.array(tmpangles)\n",
    "tmpfunc      = tmpfunc.ravel()\n",
    "tmpangles    = tmpangles.ravel()\n",
    "tmpangio     = maps_angio.ravel()\n",
    "tmpthick     = maps_thickness.ravel()\n",
    "\n",
    "#Remove outliers from functional MRI\n",
    "quant        = np.quantile(tmpfunc, [.1, .9])\n",
    "mask         = np.ones(tmpfunc.shape)\n",
    "mask[(tmpfunc>quant[1]) | (tmpfunc<quant[0])] = None \n",
    "tmpfunc      = tmpfunc[~np.isnan(mask)]\n",
    "tmpangles    = tmpangles[~np.isnan(mask)]\n",
    "tmpthick     = tmpthick[~np.isnan(mask)]\n",
    "tmpangio     = tmpangio[~np.isnan(mask)]\n",
    "plt.hist(tmpangio)\n",
    "\n",
    "print(tmp.shape, tmp2.shape)\n",
    "print(tmp,quant,mask,np.count_nonzero(mask) )\n",
    "mean_angio = binned_statistic(tmpangio, tmpfunc, statistic='mean', bins=100, range=(0,2))\n",
    "mean_thick = binned_statistic(tmpthick, tmpfunc, statistic='mean', bins=100, range=(0,5))\n",
    "mean_angle = binned_statistic(tmpangles, tmpfunc, statistic='mean', bins=100, range=(0,90))\n",
    "std_angio = binned_statistic(tmpangio, tmpfunc, statistic='std', bins=100, range=(0,2))\n",
    "std_thick = binned_statistic(tmpthick, tmpfunc, statistic='std', bins=100, range=(0,5))\n",
    "std_angle = binned_statistic(tmpangles, tmpfunc, statistic='std', bins=100, range=(0,90))\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(mean_angio.bin_edges[:-1],mean_angio.statistic, label='fMRI binned by angio')\n",
    "plt.figure()\n",
    "plt.scatter(mean_thick.bin_edges[:-1],mean_thick.statistic, label='fMRI binned by thickness')\n",
    "plt.figure()\n",
    "plt.scatter(mean_angle.bin_edges[:-1],mean_angle.statistic, label='fMRI binned by angles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
