{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code for improved fMRI-bias learning\n",
    "#OV Jan 2021\n",
    "####IMPORTS\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import numpy as np\n",
    "import keras\n",
    "import freesurfer as fs\n",
    "import scipy\n",
    "from   skimage.measure import label, regionprops\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import layers as KL\n",
    "from nibabel import freesurfer as nbfs\n",
    "#from freesurfer import deeplearn as fsd\n",
    "#OV: This imports all the local settings, e.g. subjects etc. from the header.py and netparms.py file, make your changes in there as needed\n",
    "from netparms import *\n",
    "from header_better import *\n",
    "import glob\n",
    "import losspad\n",
    "from numpy import random as npr\n",
    "#OV add matplotlib for plotting and cross checks\n",
    "#import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import string as string\n",
    "####END IMPORTS\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch generator (including noise augmentation) \n",
    "def generator(x, y, batch_size, augment_noise=.1):\n",
    "    batch_inputs = np.zeros((batch_size,)+x.shape[1:4])\n",
    "    batch_outputs = np.zeros((batch_size,)+y.shape[1:4])\n",
    "    masks = []\n",
    "    for sno in range(x.shape[0]):\n",
    "        mask_ind = np.where(x[sno,...] == 0)\n",
    "        masks.append(mask_ind)\n",
    "\n",
    "    found = 0\n",
    "    while (True):\n",
    "        sno = npr.randint(0, x.shape[0])\n",
    "        inp = x[sno,...].copy()\n",
    "        inp += npr.uniform(-augment_noise, +augment_noise)\n",
    "        inp[masks[sno]] = 0\n",
    "        batch_inputs[found,...] = inp\n",
    "        batch_outputs[found,...] = y[sno,...]\n",
    "        found = found + 1\n",
    "        if found >= batch_size:\n",
    "            yield batch_inputs, batch_outputs\n",
    "            found = 0\n",
    "\n",
    "#Normalisation of input image (usually parameterised map)\n",
    "def standardize_image(input_2d):\n",
    "\n",
    "   #Label zeros and normalise to normal distribution   \n",
    "   image_copy = np.copy(input_2d)\n",
    "   label_image = label(image_copy == 0)\n",
    "   largest_label, largest_area = None, 0\n",
    "   \n",
    "   for region in regionprops(label_image):\n",
    "      if region.area > largest_area:\n",
    "         largest_area = region.area\n",
    "         largest_label = region.label\n",
    "   \n",
    "   mask = label_image == largest_label\n",
    "   masked_image = np.ma.masked_where(mask, image_copy)\n",
    "   \n",
    "   masked_image     = masked_image - np.mean(masked_image)\n",
    "   masked_image     = masked_image / np.std(masked_image)\n",
    "   standardized_image = np.ma.getdata(masked_image)\n",
    "\n",
    "   return standardized_image\n",
    "\n",
    "#define maskes loss function \n",
    "def masked_MSE_loss(y_true, y_pred):\n",
    "    mask_value   = 0\n",
    "    mask_image   = tf.cast(y_true != 0, tf.float32)\n",
    "    squaredError = tf.math.squared_difference(y_true,y_pred)*mask_image\n",
    "    lval         = tf.reduce_mean(squaredError)\n",
    "    return lval \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IN DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Surfaces, CNN in- and outputs (functional, angio data, thickness)\n",
    "surfs_sphere   = []\n",
    "surfs_pial     = []\n",
    "surfs_inflated = []\n",
    "labels_cortex  = []\n",
    "maps_func      = []\n",
    "maps_angles    = []\n",
    "maps_thickness = []\n",
    "maps_angio     = []\n",
    "\n",
    "NumCrappyFunc  = 0\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "   #Read in surfaces and sphere for paramterisation\n",
    "   #t1 base directory with surfaces\t\n",
    "   anat_dir       = os.path.join(sdir, subject, '%s_T1_base' %(subject))\n",
    "   \n",
    "   #Initialise lists for functional and venographgy runs over hemispheres\n",
    "   labels    = []\n",
    "   funcs     = []\n",
    "   angles    = []\n",
    "   spheres   = []\n",
    "   pials     = []\n",
    "   inflated  = []\n",
    "   thickness = []\n",
    "   angios    = []\n",
    "\n",
    "   for hno, hemi in enumerate(hemis):   \n",
    "   #sphere coordinate system, lh and rh\n",
    "      fname          = os.path.join(anat_dir,'surf', '%s.sphere.reg' %(hemi))\n",
    "      sphere_surf    = fs.Surface.read(fname)\n",
    "      spheres.append(sphere_surf)    #append each hemisphere\n",
    "      #Pial surface\n",
    "      fname          = os.path.join(anat_dir,'surf', '%s.pial' % (hemi))\n",
    "      pial           = fs.Surface.read(fname)\n",
    "      pials.append(pial)        #append each hemisphere\n",
    "      #Inflafted surface\n",
    "      fname          = os.path.join(anat_dir,'surf', '%s.inflated' % (hemi))\n",
    "      infl           = fs.Surface.read(fname)\n",
    "      inflated.append(infl)        #append each hemisphere\n",
    "      #Thickness \n",
    "      fname          = os.path.join(anat_dir,'surf', '%s.thickness' %(hemi))\n",
    "      ov             = fs.Overlay.read(fname)\n",
    "      thick          = sphere_surf.parameterize(ov)\n",
    "      thickness.append(thick)  #append each hemisphere)\n",
    "      #Cortex label, use nibabel to read in, fs is deprecated\n",
    "      fname          = os.path.join(anat_dir,'label', '%s.cortex.label.mgz' %(hemi))\n",
    "      ov             = fs.Overlay.read(fname)\n",
    "      label          = sphere_surf.parameterize(ov)\n",
    "      labels.append(label)  #append each hemisphere)\n",
    "      #Angiographic projections\n",
    "      fname          = os.path.join(anat_dir,'mri', '%s.%s_AngioVeno_projfrac0.0.mgz' %(hemi,subject))\n",
    "      ov             = fs.Overlay.read(fname)\n",
    "      angio          = sphere_surf.parameterize(ov)\n",
    "      angios.append(angio)  #append each hemisphere)\n",
    "\n",
    "      #Initialise list of functional and orientation data that loops over runs\n",
    "      func  = []\n",
    "      angle = []\n",
    "\n",
    "      ### Read in functional data ####\n",
    "      for func_run in Func_runs:\n",
    "         dir_name    = os.path.join(sdir, subject, func_run)\n",
    "        \n",
    "         #Not all func files could be registered, so had to throw some out\n",
    "         fname    = os.path.join(dir_name,'%s.%s_%s_filtered_func_CoeffVar2white_projfrac0.6.mgz' %(hemi,subject,func_run))\n",
    "         if os.path.exists(fname):\n",
    "            print('Reading subject %s, functional run %s on hemisphere %s ' %(subject, func_run, hemi))\n",
    "            ov       = fs.Overlay.read(fname)\n",
    "            #2D representation of the cortex (surface parametrization where axes are \n",
    "            #longitude and colatitude), this python code generates parametrization \n",
    "            #from the spherical coordinates\n",
    "            mrisp               = sphere_surf.parameterize(ov) #standardize_image(sphere_surf.parameterize(ov))       \n",
    "#            test0               = mrisp\n",
    "#            test00              = ov.data     \n",
    "            target_shape_no_pad = mrisp.shape\n",
    "            #data_wrapped        = np.pad(mrisp.data, ((pad,pad),(0,0)), 'wrap')\n",
    "            #Append all surfaces, i.e. lh, rh, lr, rh,... don't make separate array dimension for hemispheres\n",
    "            func.append(mrisp)\n",
    "            #Read in orientation overlays\n",
    "            fname          = os.path.join(dir_name,'%s.pial2%s_angles.mgz' %(hemi,func_run))\n",
    "            ov             = fs.Overlay.read(fname)\n",
    "            tmp            = ov.data[0:,4].copy()\n",
    "            tmp[tmp>90]    = tmp[tmp>90]-180\n",
    "            tmp            = np.absolute(tmp)\n",
    "            mrisp          = sphere_surf.parameterize(tmp)\n",
    "            angle.append(mrisp)\n",
    "\n",
    "         else:\n",
    "            NumCrappyFunc = NumCrappyFunc+1\n",
    "       #end functional data\n",
    "\n",
    "      funcs.append(func) #append each hemisphere\n",
    "      angles.append(angle)\n",
    "\n",
    "   #end hemispheres\n",
    "   \n",
    "   surfs_sphere.append(spheres)\n",
    "   surfs_pial.append(pials)\n",
    "   surfs_inflated.append(inflated)\n",
    "   labels_cortex.append(labels)\n",
    "   maps_func.append(funcs) \t#append each subject [subj, hemis, func_runs]\n",
    "   maps_angles.append(angles) #append each subject    [subj, hemis, veno_runs]\n",
    "   maps_thickness.append(thickness)#append each subject    [subj, hemis, thickness]\n",
    "   maps_angio.append(angios)\n",
    "#end subjects\n",
    "\n",
    "#np.save('data_train/surfs_sphere.npy',surfs_sphere)\n",
    "#np.save('data_train/surfs_pials.npy',surfs_pial)\n",
    "#np.save('data_train/surfs_inflated.npy',surfs_inflated)\n",
    "#np.save('data_train/labels_cortex.npy',labels_cortex)\n",
    "#np.save('data_train/maps_func.npy',maps_func)\n",
    "#np.save('data_train/maps_angles.npy',maps_angles)\n",
    "#np.save('data_train/maps_thickness.npy',maps_thickness)\n",
    "#np.save('data_train/maps_angio.npy',maps_angio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfs_sphere   = np.load('data_train/surfs_sphere.npy',allow_pickle = \"true\")\n",
    "surfs_pials    = np.load('data_train/surfs_pials.npy',allow_pickle = \"true\")\n",
    "surfs_inflated = np.load('data_train/surfs_inflated.npy',allow_pickle = \"true\")\n",
    "labels_cortex  = np.load('data_train/labels_cortex.npy',allow_pickle = \"true\")\n",
    "maps_func      = np.load('data_train/maps_func.npy',allow_pickle = \"true\")\n",
    "maps_angles    = np.load('data_train/maps_angles.npy',allow_pickle = \"true\")\n",
    "maps_thickness = np.load('data_train/maps_thickness.npy',allow_pickle = \"true\")\n",
    "maps_angio     = np.load('data_train/maps_angio.npy',allow_pickle = \"true\")\n",
    "#%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test=angio.copy()\n",
    "#n,bins,patches = plt.hist(test.ravel(), bins = 'auto', alpha = 0.4, label = 'Normalised thickness')\n",
    "labels_cortex.shape\n",
    "labels_cortex = np.ones(labels_cortex.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_shape is\n",
      "(256, 512, 1)\n",
      "input_shape\n",
      "(256, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "numInputs = 1\n",
    "output_shape = (maps_func[0][0][0].shape+(1,)) \n",
    "input_shape  = (maps_func[0][0][0].shape+(numInputs,))\n",
    "target_shape = input_shape\n",
    "#target_shape = (maps_func[0][0][0].shape+(numInputs,))\n",
    "print('target_shape is') \n",
    "print(target_shape)\n",
    "print('input_shape')\n",
    "print(input_shape)\n",
    "K.clear_session()\n",
    "model = ne.models.unet(nb_features, target_shape, nb_depth, (3,3), 1, nb_conv_per_level = nb_conv_per_level, \\\n",
    "                       batch_norm = -1, final_pred_activation = 'linear')\n",
    "\n",
    "\n",
    "#Assign in- and output data\n",
    "train_inputs  = []\n",
    "train_outputs = [] \n",
    "for sno, subject in enumerate(subjects): \n",
    "   for hno, hemi in enumerate(hemis):\n",
    "      for rn, func_runs in enumerate(Func_runs):\n",
    "         if numInputs > 1:\n",
    "            #We don't have a full set of runs for all subjects, some didn't register well\n",
    "            #also multiply by cortex label \n",
    "            tmp_in = np.zeros(input_shape)\n",
    "            try:\n",
    "               train_outputs.append(np.multiply(maps_func[sno][hno][rn].data,labels_cortex[sno][hno]))\n",
    "               tmp_in[...,0] = np.multiply(maps_thickness[sno][hno].data,labels_cortex[sno][hno])\n",
    "               tmp_in[...,1] = np.multiply(maps_angio[sno][hno].data,labels_cortex[sno][hno])\n",
    "               tmp_in[...,2] = np.multiply(maps_angles[sno][hno][rn].data,labels_cortex[sno][hno])\n",
    "               train_inputs.append(tmp_in)\n",
    "            except IndexError:\n",
    "               continue\n",
    "         elif numInputs == 1:\n",
    "            try: \n",
    "               train_inputs.append(np.multiply(maps_func[sno][hno][rn].data,labels_cortex[sno][hno]))\n",
    "               #train_outputs.append(maps_angio[sno][hno].data)\n",
    "               train_outputs.append(np.multiply(maps_thickness[sno][hno].data,labels_cortex[sno][hno]))\n",
    "               #train_outputs.append(maps_angles[sno][hno][rn].data) \n",
    "               #print(\"Single input\")\n",
    "            except IndexError:\n",
    "               continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dimensions (162, 256, 512, 1)\n",
      "output dimensions(162, 256, 512, 1)\n",
      "input train dimensions (142, 256, 512, 1)\n",
      "input test dimensionss(1, 256, 512, 1)\n",
      "output train dimensions (142, 256, 512, 1)\n",
      "output test dimensionss(1, 256, 512, 1)\n",
      "using masked loss\n",
      "Epoch 1/20\n",
      "1024/1024 [==============================] - 275s 220ms/step - loss: 2.3703 - accuracy: 0.0256 - val_loss: 0.4555 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1024/1024 [==============================] - 206s 201ms/step - loss: 0.3995 - accuracy: 1.7252e-06 - val_loss: 0.4604 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1024/1024 [==============================] - 206s 201ms/step - loss: 0.3914 - accuracy: 1.0652e-06 - val_loss: 0.4895 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1024/1024 [==============================] - 206s 201ms/step - loss: 0.3838 - accuracy: 1.6141e-06 - val_loss: 0.3867 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1024/1024 [==============================] - 206s 202ms/step - loss: 0.3421 - accuracy: 7.0372e-06 - val_loss: 0.3580 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1024/1024 [==============================] - 207s 202ms/step - loss: 0.1816 - accuracy: 4.6940e-05 - val_loss: 0.3187 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      " 650/1024 [==================>...........] - ETA: 1:15 - loss: 0.1207 - accuracy: 7.8656e-05"
     ]
    }
   ],
   "source": [
    "#make directory for callbacks \n",
    "model_dir = 'model_dir'\n",
    "RandomName=''.join(rd.sample(string.ascii_lowercase,5))\n",
    "#RandomName='leave_out09_Thickness'# Thickness_Mean' #riple' #Orientation' #Thickness' #AngioVeno_projfrac0.0'\n",
    "os.mkdir(os.path.join('/cluster/visuo/users/olivia/Projects/DeepBias/Better_code',model_dir,RandomName))\n",
    "save_file_name = os.path.join(model_dir, RandomName, '{epoch:02d}.h5') \n",
    "save_callbacks = tf.keras.callbacks.ModelCheckpoint(save_file_name)\n",
    "#subclass of lr_callback written by Bruce. If loss explodes it detects that and falls back to a checkpoint\n",
    "#lr_callback    = nes.tf.callbacks.ReduceLRWithModelCheckpointAndRecovery(save_file_name)\n",
    "  \n",
    "sphereloss = losspad.spherical_loss(target_shape[0:2],pad=pad) #_no_pad[0:2],pad=pad)\n",
    "#OV: neuron (ne) expects another channel, but here we just have one number (the thickness or fmri derived parameter), \n",
    "#maybe replace that with the time series itself? for now we just add np.newaxis as an empty channel\n",
    "if numInputs > 1:\n",
    "   x = np.array(train_inputs)\n",
    "   y = np.array(train_outputs)[...,np.newaxis]\n",
    "elif numInputs == 1:\n",
    "   x = np.array(train_inputs)[...,np.newaxis]\n",
    "   y = np.array(train_outputs)[...,np.newaxis]\n",
    "\n",
    "print('input dimensions ' + str(x.shape))\n",
    "print('output dimensions' + str(y.shape))\n",
    "\n",
    "\n",
    "Total_samples = x.shape[0]\n",
    "#OV train on first subject\n",
    "xtrain = x[0:(Total_samples-20),...]\n",
    "ytrain = y[0:(Total_samples-20),...]\n",
    "xtest  = x[(Total_samples-1):Total_samples,...] #left and right hemisphere of last subject\n",
    "ytest  = y[(Total_samples-1):Total_samples,...]\n",
    "print('input train dimensions '  + str(xtrain.shape))\n",
    "print('input test dimensionss'   + str(xtest.shape))\n",
    "print('output train dimensions ' + str(ytrain.shape))\n",
    "print('output test dimensionss'  + str(ytest.shape))\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "use_mask   = True\n",
    "if use_mask == False:\n",
    "    print('NOT using masked loss')\n",
    "    #OV changed to tensorflow.keras bc it gave attribute error otherwise\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='MSE', metrics = ['accuracy'])\n",
    "else:\n",
    "    print('using masked loss')\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=masked_MSE_loss , metrics = ['accuracy'])\n",
    "\n",
    "#Fit\n",
    "#dg = generator(xtrain[...,np.newaxis], ytrain, 8,augment_noise=.3)\n",
    "dg    = generator(xtrain, ytrain, 8, augment_noise=.3)\n",
    "epochs= 20\n",
    "fhist = model.fit(dg, steps_per_epoch=1024, epochs=epochs, \\\n",
    "                  validation_data= (xtest, ytest), \\\n",
    "                  verbose=1, \\\n",
    "                  callbacks=[save_callbacks])\n",
    "loss     = fhist.history['loss']\n",
    "val_loss = fhist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ep = range(epochs)\n",
    "plt.figure()\n",
    "plt.plot(ep,loss, label='train loss')\n",
    "plt.plot(ep,val_loss,label = 'val loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#model.save(os.path.join(model_dir, RandomName) + '/final_model.h5')\n",
    "#trained_model = tf.keras.models.load_model(os.path.join(model_dir,RandomName) + '/final_model.h5'\n",
    "\n",
    "#Test on unseen data from unseen subject\n",
    "yp                 = model.predict(xtest)\n",
    "#Take out non cortex labeled areas\n",
    "yp[xtest==0]       = 0\n",
    "#Mean differene between predicition and truth\n",
    "print('Prediciting error is ' + str(np.absolute(ytest-yp).mean()))\n",
    "rand               = ytest.copy().squeeze()\n",
    "#Error for scrambled prediction\n",
    "np.random.shuffle(rand)\n",
    "print('Random error is ' + str(np.absolute(ytest.squeeze()-rand).mean()))\n",
    "#Error for scrambled predictor (input)\n",
    "bonkers           = xtest.copy().squeeze()\n",
    "np.random.shuffle(bonkers)\n",
    "bonkers           = np.array(bonkers)[np.newaxis,...,np.newaxis]\n",
    "yp_bonkers        = model.predict(bonkers)\n",
    "print('Error between bonkers input predictino and truth is ' + \\\n",
    "      str(np.absolute(yp_bonkers.squeeze() - ytest.squeeze()).mean()))\n",
    "\n",
    "\n",
    "\n",
    "#Histogram of predicition error for orientation (not in percent)\n",
    "fig = plt.figure()\n",
    "tmp = ytest-yp\n",
    "tmp = tmp[~np.isnan(tmp)]\n",
    "tmp = tmp[~np.isinf(tmp)]\n",
    "tmp2 = ytest.squeeze()-rand\n",
    "tmp2 = tmp2[~np.isnan(tmp2)]\n",
    "tmp2 = tmp2[~np.isinf(tmp2)] \n",
    "n,bins,patches = plt.hist(tmp, bins = 'auto', alpha = 0.4, label = 'Prediction')\n",
    "plt.hist(tmp2, bins = bins, alpha = 0.4, label = 'Randomised orientation')\n",
    "plt.xlabel('prediction error [\\deg]')\n",
    "plt.ylabel('counts')\n",
    "plt.xlim(-90,90)\n",
    "#plt.legend()\n",
    "plt.title('Prediction error of orientation from fMRI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere      = surfs_sphere[-1][-1]\n",
    "pial_disp   = surfs_inflated[-1][-1]\n",
    "fv          = fs.Freeview()\n",
    "truth       = sphere.sample_parameterization(ytest.squeeze())\n",
    "overlay     = fv.OverlayTag(truth, name='truth')#, threshold=(.0, 90)) \n",
    "overlays    = [overlay]\n",
    "prediction  = sphere.sample_parameterization(yp.squeeze())\n",
    "overlay     = fv.OverlayTag(prediction, name='prediction')#, threshold=(0, 90)) \n",
    "overlays.append(overlay)\n",
    "difference  = sphere.sample_parameterization(ytest.squeeze()-yp.squeeze())\n",
    "overlay     = fv.OverlayTag(difference, name='difference')#, threshold=(-50,50)) \n",
    "overlays.append(overlay)\n",
    "\n",
    "\n",
    "fv.surf(pial_disp, overlay=overlays)\n",
    "fv.show(threads=20)\n",
    "\n",
    "#fv.show.__code__.co_varnames\n",
    "\n",
    "#Plot model overview\n",
    "#keras.utils.plot_model(model,to_file='model.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(xtest)\n",
    "xtest[:,:,:,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
